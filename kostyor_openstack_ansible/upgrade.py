# This file is part of OpenStack Ansible driver for Kostyor.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# I feel incredibly wrong about these lines but it seems like the only
# working solution right now. Celery uses its own fork of native
# multiprocessing module, which is significantly diverged from the
# version of Python 2.7. So when it come to start 'multiprocessing.Process'
# instance from within Celery task, it simply fails due to inability to
# retrieve some properties (e.g. _authkey) from '_current_process' since
# they simply don't exist in 'billiard.Process.
#
# This is essential part of this driver, since Ansible internally use
# multiprocessing.Process to do parallel execution.
import multiprocessing
import billiard
multiprocessing.Process = billiard.Process  # noqa

import os
import glob

import celery

from ansible.cli.playbook import PlaybookCLI
from ansible.executor.playbook_executor import PlaybookExecutor
from ansible.inventory import Inventory
from ansible.parsing.dataloader import DataLoader
from ansible.vars import VariableManager
from ansible.utils.vars import combine_vars

from kostyor.db import api as dbapi
from kostyor.upgrades.drivers import base
from kostyor.rpc.app import app
from kostyor.rpc import tasks


class _setcwd(object):
    """Context manager for temporally changing current working directory.

    Usage example:

        with _setcwd('/opt/openstack-ansible/playbooks'):
            _run_playbook(...)

    :param cwd: current working directory to be set
    :type cwd: str
    """

    def __init__(self, cwd):
        self._newcwd = cwd
        self._oldcwd = None

    def __enter__(self):
        self._oldcwd = os.getcwd()

        if self._newcwd:
            os.chdir(self._newcwd)

    def __exit__(self, *args):
        if self._newcwd:
            os.chdir(self._oldcwd)


def _get_user_settings(loader):
    settings = {}

    # /etc/openstack_deploy is default and, by all means, hardcoded path
    # to deployment settings. The dir contains user settings, where each
    # file starts with 'user_' prefix and ends with '.yml' suffix.
    pattern = os.path.join('/etc', 'openstack_deploy', 'user_*.yml')

    for filename in glob.glob(pattern):
        # Ansible may use different strategies of combining variables, so
        # we need to use its function instead of '.update(...)' method.
        settings = combine_vars(settings, loader.load_from_file(filename))

    return settings


def _get_component_from_service(service):
    # OpenStack services has the following naming format: '{component}-*',
    # so we can use first part before dash as a component name.
    return service['name'].split('-')[0]


def _get_component_hosts_on_node(inventory, service, host):
    component = _get_component_from_service(service)
    variables = inventory.get_vars(host['hostname'])
    hostgroup = inventory.get_group(variables['container_types'])

    # Despite the fact that 'container_types' host variable always exists,
    # it may points to non-existing group. For instance, compute hosts
    # have 'container_types=computeX-host_containers' but the group
    # doesn't exist within inventory.
    if hostgroup is not None:
        containers = hostgroup.get_hosts()
    else:
        containers = []

    # Not all services are running in containers, so we want to add the
    # host itself into the list.
    hosts = containers + inventory.get_hosts(host['hostname'])
    service_hosts = inventory.get_group(component + '_all').get_hosts()

    # Herer's the trick: intersection between node hosts and service hosts
    # (which include hosts from different nodes) gives ua only service
    # hosts on a given node.
    return list(set(hosts) & set(service_hosts))


def _run_playbook(playbook, hosts_fn=None, cwd=None, ignore_errors=False):
    # Unfortunately, there's no good way to get the options instance
    # with proper defaults since it's generated by argparse inside
    # PlaybookCLI. Due to the fact that the options can't be empty
    # and must contain proper values we have not choice but extract
    # them from PlaybookCLI instance.
    playbook_cli = PlaybookCLI(['to-be-stripped', playbook])
    playbook_cli.parse()
    options = playbook_cli.options

    # Get others required options.
    loader = DataLoader()
    variable_manager = VariableManager()
    inventory = Inventory(loader, variable_manager)
    variable_manager.set_inventory(inventory)
    variable_manager.extra_vars = _get_user_settings(loader)

    # Limit playbook execution to hosts returned by 'hosts_fn'.
    if hosts_fn is not None:
        inventory.subset([
            host.get_vars()['inventory_hostname']
            for host in hosts_fn(inventory)
        ])

    # Finally, we can create a playbook executor and run the playbook.
    executor = PlaybookExecutor(
        playbooks=[playbook],
        inventory=inventory,
        variable_manager=variable_manager,
        loader=loader,
        options=options,
        passwords={}
    )

    # Some playbooks may rely on current working directory, so better allow
    # to change it before execution.
    with _setcwd(cwd):
        exitcode = executor.run()

    # Celery treats exceptions from task as way to mark it failed. So let's
    # throw one to do so in case return code is not zero.
    if all([not ignore_errors, exitcode is not None, exitcode != 0]):
        raise Exception('Playbook "%s" has been finished with errors. '
                        'Exit code is "%d".' % (playbook, exitcode))

    return exitcode


@app.task
def run_playbook(playbook, cwd=None, ignore_errors=False):
    return _run_playbook(playbook, cwd=cwd, ignore_errors=ignore_errors)


@app.task
def run_playbook_for(playbook, host, service, cwd=None, ignore_errors=False):
    return _run_playbook(
        playbook,
        lambda inv: _get_component_hosts_on_node(inv, service, host),
        cwd=cwd,
        ignore_errors=ignore_errors
    )


class Driver(base.UpgradeDriver):
    """Upgrade driver implementation for OpenStack Ansible.

    As any upgrade driver returns a Celery task to be executed in order to
    achieve some goal (start upgrade, stop upgrade, etc). In order to have
    a successful execution it's mandatory to run those tasks on deployment
    host, so make sure you run your Celery worker there.

    Since OpenStack Ansible Newton we have the 'openstack-ansible.rc' shell
    script that must be sourced before running OpenStack Ansible and defines
    some Ansible env variables such as path to inventory. In order to avoid
    unnecessary complications in driver and to support as many versions as
    possible, it's up to end user to ensure that Celery worker is running
    with those env variables.

    There's another important note, hostnames in Kostyor's database must
    be the same used by OpenStack Ansible. So far we don't support
    mapping by IP address or something like that.
    """

    #: Kostyor is designed to have an upgrade resolution up to microservice.
    #: Unfortunately, OpenStack Ansible provides a resolution up to service,
    #: so technically we have the very same playbook for the whole list of
    #: service's subservices.
    _playbooks = {
        'keystone-wsgi-admin':       'os-keystone-install.yml',
        'keystone-wsgi-public':      'os-keystone-install.yml',

        'glance-api':                'os-glance-install.yml',
        'glance-registry':           'os-glance-install.yml',

        'nova-conductor':            'os-nova-install.yml',
        'nova-scheduler':            'os-nova-install.yml',
        'nova-cells':                'os-nova-install.yml',
        'nova-cert':                 'os-nova-install.yml',
        'nova-console':              'os-nova-install.yml',
        'nova-consoleauth':          'os-nova-install.yml',
        'nova-network':              'os-nova-install.yml',
        'nova-novncproxy':           'os-nova-install.yml',
        'nova-serialproxy':          'os-nova-install.yml',
        'nova-spicehtml5proxy':      'os-nova-install.yml',
        'nova-xvpvncproxy':          'os-nova-install.yml',
        'nova-api':                  'os-nova-install.yml',
        'nova-api-metadata':         'os-nova-install.yml',
        'nova-api-os-compute':       'os-nova-install.yml',
        'nova-compute':              'os-nova-install.yml',

        'neutron-server':            'os-neutron-install.yml',
        'neutron-openvswitch-agent': 'os-neutron-install.yml',
        'neutron-linuxbridge-agent': 'os-neutron-install.yml',
        'neutron-sriov-nic-agent':   'os-neutron-install.yml',
        'neutron-l3-agent':          'os-neutron-install.yml',
        'neutron-dhcp-agent':        'os-neutron-install.yml',
        'neutron-metering-agent':    'os-neutron-install.yml',
        'neutron-metadata-agent':    'os-neutron-install.yml',
        'neutron-ns-metadata-proxy': 'os-neutron-install.yml',

        'cinder-api':                'os-cinder-install.yml',
        'cinder-scheduler':          'os-cinder-install.yml',

        'heat-api':                  'os-heat-install.yml',
        'heat-engine':               'os-heat-install.yml',
        'heat-api-cfn':              'os-heat-install.yml',
        'heat-api-cloudwatch':       'os-heat-install.yml',

        'horizon-wsgi':              'os-horizon-install.yml',
    }

    # A path to OpenStack Ansible sources.
    #
    # TODO: to be configurable
    _root = os.path.join('/opt', 'openstack-ansible')

    def __init__(self, *args, **kwargs):
        super(Driver, self).__init__(*args, **kwargs)

        #: Due to the fact that we have one playbook that upgrades the whole
        #: service (multiple microservices at once), we will upgrade them all
        #: on the same host when firing, for instance, nova-api. In order
        #: to prevent running this playbook once again (it makes no sense),
        #: we need to track playbook executions per host.
        #:
        #: The dict has the following format:
        #:
        #:   (host, playbook) -> is-executed
        self._executions = {}

    def pre_upgrade_hook(self, upgrade_task):
        utilities = os.path.join(
            self._root, 'scripts', 'upgrade-utilities', 'playbooks')
        playbooks = os.path.join(self._root, 'playbooks')

        # According to the upgrade document, there are steps that must be
        # executed before trying to upgrade OpenStack to new version. This
        # this hook returns a chain of this steps so operator doesn't need
        # to run them manually.
        #
        # http://docs.openstack.org/developer/openstack-ansible/upgrade-guide/manual-upgrade.html
        return celery.chain(*[

            # Bootstrapping Ansible again ensures that all OpenStack Ansible
            # role dependencies are in place before running playbooks of new
            # release.
            tasks.execute.si(
                os.path.join(self._root, 'scripts', 'bootstrap-ansible.sh'),
                cwd=self._root,
            ),

            # Some configuration may changed, and old facts should be purged.
            run_playbook.si(
                os.path.join(utilities, 'ansible_fact_cleanup.yml'),
            ),

            # The user configuration files in /etc/openstack_deploy/ and
            # the environment layout in /etc/openstack_deploy/env.d may
            # have new name values added in new release.
            run_playbook.si(
                os.path.join(utilities, 'deploy-config-changes.yml'),
            ),

            # Populate user_secrets.yml with new secrets added in new
            # release.
            run_playbook.si(
                os.path.join(utilities, 'user-secrets-adjustment.yml'),
            ),

            # The presence of pip.conf file can cause build failures when
            # upgrading. So better remove it everywhere.
            run_playbook.si(
                os.path.join(utilities, 'pip-conf-removal.yml'),
            ),

            # Update the configuration of the repo servers and build a new
            # packages required by new release.
            run_playbook.si(
                os.path.join(playbooks, 'repo-install.yml'),
                cwd=playbooks,
            ),
        ])

    def start_upgrade(self, upgrade_task, service):
        # Kostyor's model may contain information about more services
        # than we support. It seems reasonable to do not fail on such
        # services and skip it for now.
        if service['name'] not in self._playbooks:
            return tasks.noop.si()

        # If playbook was executed on the host, then do nothing since.
        # This is a general case for this driver as long as one playbook
        # upgrades the whole service at once.
        key = service['host_id'], self._playbooks[service['name']]
        if self._executions.get(key):
            return tasks.noop.si()
        self._executions[key] = True

        return run_playbook_for.si(
            os.path.join(
                self._root, 'playbooks', self._playbooks[service['name']]
            ),

            # By default, OpenStack Ansible deploys control plane services
            # in LXC containers, and use those as hosts in Ansible inventory.
            # However, from Kostyor's point of view we are interested in
            # baremetal node-by-node upgrade and we don't want to know about
            # containers. So we need to limit playbook execution only to
            # a baremetal node and its containers.
            dbapi.get_host(service['host_id']),
            service,
        )

    def stop_upgrade(self, upgrade_task, service):
        raise NotImplementedError()

    def pause_upgrade(self, upgrade_task, service):
        raise NotImplementedError()

    def continue_upgrade(self, upgrade_task, service):
        raise NotImplementedError()

    def cancel_upgrade(self, upgrade_task, service):
        raise NotImplementedError()

    def rollback_upgrade(self, upgrade_task, service):
        raise NotImplementedError()

    def supports_upgrade_rollback(self):
        return False
